# Awesome-LLM-Hallucination-Detection-and-Mitigation




## Hallucination Detection 

### Fact-checking 

### Uncertainty Analysis 

- [Snyder2024] On Early Detection of Hallucinations in Factual Question Answering in *KDD*, 2024.[\[paper\]](https://arxiv.org/pdf/2312.14183)[\[code\]](https://github.com/amazon-science/llm-hallucinations-factual-qa) 

### Consistency Measure 


- [MÃ¼ndler2024] Self-Contradictory Hallucinations of LLMs: Evaluation, Detection and Mitigation in *ICLR*, 2024.[\[paper\]](https://arxiv.org/pdf/2305.15852)[\[code\]](https://chatprotect.ai/) 

- [Niu2025] Robust Hallucination Detection in LLMs via Adaptive Token Selection in *Arxiv*, 2025.[\[paper\]](https://arxiv.org/abs/2504.07863)[\[code\]]()

- [Wang2025]  Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation in *ICLR*, 2025.[\[paper\]](https://arxiv.org/abs/2410.13640)[\[code\]](https://github.com/Alsace08/Chain-of-Embedding) 

## Hallucination Mitigation 


### Data Preprocessing 





### Model Calibration 

- [Liu2023] LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses in *ICLR*,2023.  [\[paper\]](https://arxiv.org/abs/2310.19208)[\[code\]](https://github.com/launchnlp/LitCab)

- [Zhang2024] R-Tuning: Instructing Large Language Models to Say `I Don't Know' in *NAACL*,  2023.  [\[paper\]](https://arxiv.org/abs/2311.09677)[\[code\]](https://github.com/shizhediao/R-Tuning)

- [Chuang2024] DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models in *ICLR*, 2024. [\[paper\]](https://arxiv.org/abs/2309.03883)[\[code\]](https://github.com/voidism/DoLa)

- [Zhou2025] HaDeMiF: Hallucination Detection and Mitigation in Large Language Models in *ICLR*, 2025. [\[paper\]](https://openreview.net/pdf?id=VwOYxPScxB)[\[code\]]()

- [Zhang2025] The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination in *ACL*, 2025. [\[paper\]](https://arxiv.org/abs/2502.16143)[\[code\]]()


### External Knowledge 

- [Sun2025] Redeep: Detecting hallucination in retrieval-augmented generation via mechanistic interpretability  in *ICLR*, 2025. [\[paper\]](https://arxiv.org/pdf/2410.11414)[\[code\]](https://github.com/Jeryi-Sun/ReDEeP-ICLR)







# Related Survey 

- [Wang2023] Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity in *Arxiv*,2023. [\[paper\]](https://arxiv.org/abs/2310.07521) 
  
- [Ye2023] Cognitive Mirage: A Review of Hallucinations in Large Language Models  in *Arxiv*,2023. [\[paper\]](https://arxiv.org/abs/2309.06794v1) 
  
- [Zhang2023] Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models in *Arxiv*,2023. [\[paper\]](https://arxiv.org/abs/2309.01219)

- [Huang2024] A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions in *TOIS*, 2024. [\[paper\]](https://arxiv.org/pdf/2410.11414)

- [Ji2024] Survey of Hallucination in Natural Language Generation in *CSUR*, 2024.  [\[paper\]](https://arxiv.org/abs/2202.03629)




