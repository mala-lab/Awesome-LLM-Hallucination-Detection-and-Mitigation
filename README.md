# Awesome-LLM-Hallucination-Detection-and-Mitigation




## Hallucination Detection 

### Fact-checking 

### Uncertainty Analysis 

### Consistency Measure 

- [Niu2025] Robust Hallucination Detection in LLMs via Adaptive Token Selection in *Arxiv*, 2025.[\[paper\]](https://arxiv.org/abs/2504.07863)[\[code\]]()

## Hallucination Mitigation 

### Model Calibration 

- [Liu2023] LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses in *ICLR* 2023.  [\[paper\]](https://arxiv.org/abs/2310.19208)[\[code\]](https://github.com/launchnlp/LitCab)

- [Zhang2024] R-Tuning: Instructing Large Language Models to Say `I Don't Know' in *NAACL* 2023.  [\[paper\]](https://arxiv.org/abs/2311.09677)[\[code\]](https://github.com/shizhediao/R-Tuning)

- [Chuang2024] DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models in *ICLR* 2024. [\[paper\]](https://arxiv.org/abs/2309.03883)[\[code\]](https://github.com/voidism/DoLa)

- [Zhou2025] HaDeMiF: Hallucination Detection and Mitigation in Large Language Models in *ICLR* 2025. [\[paper\]](https://openreview.net/pdf?id=VwOYxPScxB)[\[code\]]()
